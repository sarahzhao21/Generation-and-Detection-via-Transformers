# Generation-and-Detection-via-Transformers

Pretrained language models like BERT and GPT have revolutionized many areas of NLP. These models have been trained on massive volumes of text and their parameters reflect a basic understanding of the structure and meaning of many kinds of text. The huge advantage for such pre-training is that models can be quickly adapted to new tasks by performing fine-tuning, where the parameters are modified to reflect a particular type of text (e.g., social media) and to use them to perform some specific task (e.g., classification). As a result, with just a limited amount of data, these models are capable of generalizing to a much wider set of instances; for example, a BERT-based classifier trained on a few thousand classifiers might perform substantially better than a logistic regression classifier trained on the same data . 

In this project, I tried to build language models that could generate text, eg. lyrics. The main tasks include:
1. Generate the lyrics to a song
2. Detect whether a song youâ€™ve been given was generated by a machine or not.

I worked with one of the many libraries built on top of the HuggingFace transformers library (SimpleTransformers), which has implementations of many of the latest-and-greatest NLP models and effectively fine-tune pretrained models to do both of these tasks.
